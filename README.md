# AI Papers of The Week

As a researcher, I frequently read AI papers and am trying to create this repository to record some highlighted AI papers that I read each week.

Here is the weekly series:

## 2024

- [AI Papers of the Week (July 8 - July 14)](./#ai-papers-of-the-week-july-8---july-14---2024)
- [AI Papers of the Week (July 1 - July 7)](./#ai-papers-of-the-week-july-1---july-7---2024)
- [AI Papers of the Week (June 24 - June 30)](./#ai-papers-of-the-week-june-24---june-30---2024)
- [AI Papers of the Week (June 17 - June 23)](./#ai-papers-of-the-week-june-17---june-23---2024)
- [AI Papers of the Week (June 10 - June 16)](./#ai-papers-of-the-week-june-10---june-16---2024)
- [AI Papers of the Week (June 3 - June 9)](./#ai-papers-of-the-week-june-3---june-9---2024)
- [AI Papers of the Week (May 27 - June 2)](./#ai-papers-of-the-week-may-27---june-2---2024)
- [AI Papers of the Week (May 20 - May 26)](./#ai-papers-of-the-week-may-20---may-26---2024)
- [AI Papers of the Week (May 13 - May 19)](./#ai-papers-of-the-week-may-13---may-19---2024)
- [AI Papers of the Week (May 6 - May 12)](./#ai-papers-of-the-week-may-6---may-12---2024)
- [AI Papers of the Week (April 29 - May 5)](./#ai-papers-of-the-week-april-29---may-5---2024)
- [AI Papers of the Week (April 22 - April 28)](./#ai-papers-of-the-week-april-22---april-28---2024)
- [AI Papers of the Week (April 15 - April 21)](./#ai-papers-of-the-week-april-15---april-21---2024)



## AI Papers of the Week (July 8 - July 14) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output** - This paper presents InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. | July 3, 2024 | [Paper](https://arxiv.org/abs/2407.03320), [Repo](https://github.com/InternLM/InternLM-XComposer)|
| (2) **MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation** - This paper proposes MMEvalPro, a benchmark designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process. MMEvalPro comprises 2,138 question triplets, totaling 6,414 distinct questions. Two-thirds of these questions are manually labeled by human experts, while the rest are sourced from existing benchmarks (MMMU, ScienceQA, and MathVista). Compared with the existing benchmarks, the experiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more challenging (the best LMM lags behind human performance by 31.73%, compared to an average gap of 8.03% in previous benchmarks) and more trustworthy (the best LLM trails the best LMM by 23.09%, whereas the gap for previous benchmarks is just 14.64%). The in-depth analysis explains the reason for the large performance gap and justifies the trustworthiness of evaluation, underscoring its significant potential for advancing future research. | June 29, 2024 | [Paper](https://arxiv.org/abs/2407.00468), [Repo](https://github.com/chenllliang/MMEvalPro)|
| (3) **Unveiling Encoder-Free Vision-Language Models** - This paper bridges the gap between encoder-based and encoder-free models, and present a simple yet effective training recipe towards pure VLMs. Specifically, the authors unveil the key aspects of training encoder-free VLMs efficiently via thorough experiments: (1) Bridging vision-language representation inside one unified decoder; (2) Enhancing visual recognition capability via extra supervision. With these strategies, the authors launch EVE, an encoder-free vision-language model that can be trained and forwarded efficiently. Notably, solely utilizing 35M publicly accessible data, EVE can impressively rival the encoder-based VLMs of similar capacities across multiple vision-language benchmarks. It significantly outperforms the counterpart Fuyu-8B with mysterious training procedures and undisclosed training data. The authors believe that EVE provides a transparent and efficient route for developing a pure decoder-only architecture across modalities. | June 17, 2024 | [Paper](https://arxiv.org/abs/2406.11832), [Repo](https://github.com/baaivision/EVE)|
| (4) **Vision language models are blind** - This paper shows that while large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini 1.5 Pro, are powering various image-text applications and scoring high on many vision-understanding benchmarks, the authors find that they are surprisingly still struggling with low-level vision tasks that are easy to humans. Specifically, on BlindTest, our suite of 7 very simple tasks such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting circles in an Olympic-like logo, four state-of-the-art VLMs are only 58.57% accurate on average. Claude 3.5 Sonnet performs the best at 74.94% accuracy, but this is still far from the human expected accuracy of 100%. Across different image resolutions and line widths, VLMs consistently struggle with tasks that require precise spatial information and recognizing geometric primitives that overlap or are close together. | July 9, 2024 | [Paper](https://arxiv.org/abs/2407.06581), [Repo](https://github.com/anguyen8/vision-llms-are-blind)|
| (5) **Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model** - This paper designs a multi-modal selfinstruct pipeline, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. This strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs like Claude-3.5-Sonnet and GPT-4o in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of the synthetic data, the authors fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks. | July 9, 2024 | [Paper](https://arxiv.org/abs/2407.07053), [Repo](https://github.com/zwq2018/Multi-modal-Self-instruct)|
| (6) **Self-Recognition in Language Models** - This paper proposes a novel approach for assessing self-recognition in LMs using model-generated "security questions". This test can be externally administered to keep track of frontier models as it does not require access to internal model parameters or output probabilities. The authors use this test to examine self-recognition in ten of the most capable open- and closed-source LMs currently publicly available. The extensive experiments found no empirical evidence of general or consistent self-recognition in any examined LM. Instead, the results suggest that given a set of alternatives, LMs seek to pick the "best" answer, regardless of its origin. Moreover, the authors find indications that preferences about which models produce the best answers are consistent across LMs. The authors additionally uncover novel insights on position bias considerations for LMs in multiple-choice settings. | July 9, 2024 | [Paper](https://arxiv.org/abs/2407.06946), [Repo](https://github.com/trdavidson/self-recognition)|
| (7) **PaliGemma: A versatile 3B VLM for transfer** - This paper introduces PaliGemma, an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. The authors evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation. | July 10, 2024 | [Paper](https://arxiv.org/abs/2407.07726)|


## AI Papers of the Week (July 1 - July 7) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Fantastic Copyrighted Beasts and How (Not) to Generate Them** - This paper builds CopyCat, an evaluation suite consisting of diverse copyrighted characters and a novel evaluation pipeline. This evaluation considers both the detection of similarity to copyrighted characters and generated image's consistency with user input. The evaluation systematically shows that both image and video generation models can still generate characters even if characters' names are not explicitly mentioned in the prompt, sometimes with only two generic keywords (e.g., prompting with "videogame, plumber" consistently generates Nintendo's Mario character). The authors then introduce techniques to semi-automatically identify such keywords or descriptions that trigger character generation. Using this evaluation suite, the authors study runtime mitigation strategies, including both existing methods and new strategies the authors propose. The findings reveal that commonly employed strategies, such as prompt rewriting in the DALL-E system, are not sufficient as standalone guardrails. These strategies must be coupled with other approaches, like negative prompting, to effectively reduce the unintended generation of copyrighted characters. This work provides empirical grounding to the discussion of copyright mitigation strategies and offers actionable insights for model deployers actively implementing them. | June 20, 2024 | [Paper](https://arxiv.org/abs/2406.14526), [Repo](https://github.com/princeton-nlp/CopyCat)|
| (2) **SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation** - This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. The experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods. | June 27, 2024 | [Paper](https://arxiv.org/abs/2406.19215), [Repo](https://github.com/THU-KEG/SeaKR)|
| (3) **LiveBench: A Challenging, Contamination-Free LLM Benchmark** - This paper introduces a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. The authors release LiveBench, the first benchmark that (1) contains frequently updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. The authors evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65% accuracy. The authors release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and the authors will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. | June 27, 2024 | [Paper](https://arxiv.org/abs/2406.19314), [Repo](https://github.com/livebench/livebench)|
| (4) **Can LLMs Learn by Teaching? A Preliminary Study** - This paper shows that the ideas of Can LLMs also learn by teaching (LbT) can be incorporated into existing LLM training/prompting pipelines and provide noticeable improvements. Specifically, the authors design three methods, each mimicking one of the three levels of LbT in humans: observing students’ feedback, learning from the feedback, and learning iteratively, with the goals of improving answer accuracy without training and improving models’ inherent capability with fine-tuning. The findings are encouraging. For example, similar to LbT in human, the authors see that: (1) LbT can induce weak-to-strong generalization: strong models can improve themselves by teaching other weak models; (2) Diversity in students might help: teaching multiple students could be better than teaching one student or the teacher itself. the authors hope that this early promise can inspire future research on LbT and more broadly adopting the advanced techniques in education to improve LLMs. | June 20, 2024 | [Paper](https://arxiv.org/abs/2406.14629), [Repo](https://github.com/imagination-research/lbt)|
| (5) **Scaling Synthetic Data Creation with 1,000,000,000 Personas** - This paper proposes a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, the authors introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, the authors demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development. | June 28, 2024 | [Paper](https://arxiv.org/abs/2406.20094), [Repo](https://github.com/tencent-ailab/persona-hub)|
| (6) **Aligning Teacher with Student Preferences for Tailored Training Data Generation** - This paper proposes ARTE, dubbed Aligning TeacheR with StudenT PreferencEs, a framework that aligns the teacher model with student preferences to generate tailored training examples for Knowledge Distillation. Specifically, the authors elicit draft questions and rationales from the teacher model, then collect student preferences on these questions and rationales using students' performance with in-context learning as a proxy, and finally align the teacher model with student preferences. In the end, the authors repeat the first step with the aligned teacher model to elicit tailored training examples for the student model on the target task. Extensive experiments on academic benchmarks demonstrate the superiority of ARTE over existing instruction-tuning datasets distilled from powerful LLMs. Moreover, the authors thoroughly investigate the generalization of ARTE, including the generalization of fine-tuned student models in reasoning ability and the generalization of aligned teacher models to generate tailored training data across tasks and students. In summary, the contributions lie in proposing a novel framework for tailored training example generation, demonstrating its efficacy in experiments, and investigating the generalization of both student & aligned teacher models in ARTE. | June 28, 2024 | [Paper](https://arxiv.org/abs/2406.19227), [Repo](https://github.com/THU-KEG/ARTE)|
| (7) **Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs** - This paper introduces Chain-of-Knowledge, a comprehensive framework for knowledge reasoning, including methodologies for both dataset construction and model learning. For dataset construction, the authors create KnowReason via rule mining on KGs. For model learning, the authors observe rule overfitting induced by naive training. Hence, the authors enhance CoK with a trial-and-error mechanism that simulates the human process of internal knowledge exploration. The authors conduct extensive experiments with KnowReason. The results show the effectiveness of CoK in refining LLMs in not only knowledge reasoning, but also general reasoning benchmarks. | June 30, 2024 | [Paper](https://arxiv.org/abs/2407.00653)|


## AI Papers of the Week (June 24 - June 30) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching** - This paper motivated by the remarkable success of the Feynman Technique in efficient human learning, the authors introduce SELFTUNING, a learning framework aimed at improving an LLM’s ability to effectively acquire new knowledge from raw documents through self-teaching. Specifically, the authors develop a SELFTEACHING strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. In addition, the authors introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM’s knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on LLAMA2 family models reveal that SELF-TUNING consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge. | June 15, 2024 | [Paper](https://arxiv.org/abs/2406.06326)|
| (2) **Unlocking Continual Learning Abilities in Language Models** - This paper introduces MIGU (MagnItude-based Gradient Updating for continual learning), a rehearsal-free and task-label-free method that only updates the model parameters with large magnitudes of output in LMs' linear layers. MIGU is based on our observation that the L1-normalized magnitude distribution of the output in LMs' linear layers is different when the LM models deal with different task data. By imposing this simple constraint on the gradient update process, we can leverage the inherent behaviors of LMs, thereby unlocking their innate CL abilities. The experiments demonstrate that MIGU is universally applicable to all three LM architectures (T5, RoBERTa, and Llama2), delivering state-of-the-art or on-par performance across continual finetuning and continual pre-training settings on four CL benchmarks. For example, MIGU brings a 15.2% average accuracy improvement over conventional parameter-efficient finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly integrate with all three existing CL types to further enhance performance. | June 25, 2024 | [Paper](https://arxiv.org/abs/2406.17245), [Repo](https://github.com/wenyudu/MIGU)|
| (3) **Scalable MatMul-free Language Modeling** - This paper shows that Matrix multiplication (MatMul) operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Experiments show that the proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. The authors investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. The authors also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, the model’s memory consumption can be reduced by more than 10× compared to unoptimized models. To properly quantify the efficiency of our architecture, the authors build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. The authors processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. | June 18, 2024 | [Paper](https://arxiv.org/abs/2406.02528), [Repo](https://github.com/ridgerchu/matmulfreellm)|
| (4) **LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs** - This paper proposes a new framework LongRAG, consisting of a ‘long retriever’ and a ‘long reader’. LongRAG processes the entire Wikipedia into 4K-token units, which is 30x longer than before. By increasing the unit size, the authors significantly reduce the total units from 22M to 600K. This significantly lowers the burden of retriever, which leads to a remarkable retrieval score: answer recall@1=71% on NQ (previously 52%) and answer recall@2=72% (previously 47%) on HotpotQA (full-wiki). Then the authors feed the top-k retrieved units (≈ 30K tokens) to an existing long-context LLM to perform zero-shot answer extraction. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA (full-wiki), which is on par with the (fully-trained) SoTA model. The study offers insights into the future roadmap for combining RAG with long-context LLMs. | June 30, 2024 | [Paper](https://arxiv.org/abs/2406.15319), [Repo](https://github.com/TIGER-AI-Lab/LongRAG/)|
| (5) **Long Context Transfer from Language to Vision** - This paper approaches the problem of existing large multimodal models (LMMs) fall short in understanding extremely long videosfrom the perspective of the language model. By simply extrapolating the context length of the language backbone, the authors enable LMMs to comprehend orders of magnitude more visual tokens without any video training. The authors call this phenomenon long context transfer and carefully ablate its properties. To effectively measure LMMs' ability to generalize to long contexts in the vision modality, the authors develop V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model's NIAH test. Our proposed Long Video Assistant (LongVA) can process 2000 frames or over 200K visual tokens without additional complexities. With its extended context length, LongVA achieves state-of-the-art performance on Video-MME among 7B-scale models by densely sampling more input frames. | June 24, 2024 | [Paper](https://arxiv.org/abs/2406.16852), [Repo](https://github.com/EvolvingLMMs-Lab/LongVA)|
| (6) **Octo-planner: On-device Language Model for Planner-Action Agents** - This paper presents an efficient on-device Planner-Action framework that separates planning and action execution into two components: a planner agent, or Octo-planner, optimized for edge devices, and an action agent using the Octopus model for function execution. Octo-planner first responds to user queries by decomposing tasks into a sequence of sub-steps, which are then executed by the Octopus action agent. To optimize performance on resource-constrained devices, the authors employ model fine-tuning instead of in-context learning, reducing computational costs and energy consumption while improving response times. This approach involves using GPT-4 to generate diverse planning queries and responses based on available functions, with subsequent validations to ensure data quality. the authors fine-tune the Phi-3 Mini model on this curated dataset, achieving a 97% success rate in our in-domain test environment. To address multidomain planning challenges, the authors developed a multi-LoRA training method that merges weights from LoRAs trained on distinct function subsets. This approach enables flexible handling of complex, multi-domain queries while maintaining computational efficiency on resource-constrained devices. | June 26, 2024 | [Paper](https://arxiv.org/abs/2406.18082), [Repo](https://huggingface.co/NexaAIDev/octopus-planning)|
| (7) **WILDGUARD: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs** - This paper introduce WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. To address these challenges, the authors construct WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios. Through extensive evaluations on WildGuardTest and ten existing public benchmarks, the authors show that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8% to 2.4%. | June 26, 2024 | [Paper](https://arxiv.org/abs/2406.18495), [Repo](https://github.com/allenai/wildguard)|


## AI Papers of the Week (June 17 - June 23) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning** - This paper introduces Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state. The authors identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions. The experiments show that Husky outperforms prior language agents across 14 evaluation datasets. Moreover, the authors introduces HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning. Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems. | June 10, 2024 | [Paper](https://arxiv.org/abs/2406.06469), [Repo](https://github.com/agent-husky/Husky-v1)|
| (2) **Needle In A Multimodal Haystack** - This paper presents Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. This benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, the authors observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. The authors hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. | June 11, 2024 | [Paper](https://arxiv.org/abs/2406.07230), [Repo](https://github.com/OpenGVLab/MM-NIAH)|
| (3) **Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing** - This paper presents a self-synthesis method for generating large-scale alignment data named MAGPIE. The key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when authors input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature. The authors use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses. The authors perform a comprehensive analysis of the extracted data and select 300K high-quality instances. To compare MAGPIE data with other public instruction datasets (e.g., ShareGPT, WildChat, Evol-Instruct, UltraChat, OpenHermes, Tulu-V2-Mix), the authors fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models. The results indicate that in some tasks, models fine-tuned with MAGPIE perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning (SFT) and subsequent feedback learning. The authors also show that using MAGPIE solely for SFT can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback. This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench, and importantly, it is achieved without compromising performance on reasoning tasks like MMLU-Redux, despite the alignment tax. | June 12, 2024 | [Paper](https://arxiv.org/abs/2406.08464), [Repo](https://github.com/magpie-align/magpie)|
| (4) **DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence** - This paper presents DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks. | June 17, 2024 | [Paper](https://arxiv.org/abs/2406.11931), [Repo](https://github.com/deepseek-ai/DeepSeek-Coder-V2)|
| (5) **Adversarial Attacks on Multimodal Agents** - This paper shows that multimodal agents raise new safety risks, even though attacking agents is more challenging than prior attacks due to limited access to and knowledge about the environment. The author's attacks use adversarial text strings to guide gradient-based perturbation over one trigger image in the environment: (1) the captioner attack attacks white-box captioners if they are used to process images into captions as additional inputs to the VLM; (2) the CLIP attack attacks a set of CLIP models jointly, which can transfer to proprietary VLMs. To evaluate the attacks, the authors curated VisualWebArena-Adv, a set of adversarial tasks based on VisualWebArena, an environment for web-based multimodal agent tasks. Within an L-infinity norm of 16/256 on a single image, the captioner attack can make a captioner-augmented GPT-4V agent execute the adversarial goals with a 75% success rate. When the authors remove the captioner or use GPT-4V to generate its own captions, the CLIP attack can achieve success rates of 21% and 43%, respectively. Experiments on agents based on other VLMs, such as Gemini-1.5, Claude-3, and GPT-4o, show interesting differences in their robustness. Further analysis reveals several key factors contributing to the attack's success, and the authors also discuss the implications for defenses as well. | June 18, 2024 | [Paper](https://arxiv.org/abs/2406.12814), [Repo](https://github.com/ChenWu98/agent-attack)|
| (6) **Instruction Pre-Training: Language Models are Supervised Multitask Learners** - This paper explores supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In the experiments, the authors synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3-70B. | June 20, 2024 | [Paper](https://arxiv.org/abs/2406.14491), [Repo](https://github.com/microsoft/LMOps)|
| (7) **Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs** - This paper presents Prism, an innovative framework designed to disentangle the perception and reasoning processes involved in visual question solving. Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM). This modular design enables the systematic comparison and assessment of both proprietary and open-source VLM for their perception and reasoning strengths. The analytical framework provides several valuable insights, underscoring Prism's potential as a cost-effective solution for vision-language tasks. By combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses. Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs 10× larger on the rigorous multimodal benchmark MMStar.  | June 20, 2024 | [Paper](https://arxiv.org/abs/2406.14544), [Repo](https://github.com/SparksJoe/Prism)|


## AI Papers of the Week (June 10 - June 16) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild** - This paper introduces WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WILDBENCH consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WILDBENCH, the authors have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WILDBENCH evaluation uses taskspecific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, the authors selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, the authors propose a simple method to mitigate length bias, by converting outcomes of “slightly better/worse” to “tie” if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WILDBENCH results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard’s 0.91 and AlpacaEval2.0’s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates. | June 7, 2024 | [Paper](https://arxiv.org/abs/2406.04770), [Repo](https://github.com/allenai/WildBench)|
| (2) **Mixture-of-Agents Enhances Large Language Model Capabilities** - This paper proposes a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In this approach, the authors construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, the MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni. | June 7, 2024 | [Paper](https://arxiv.org/abs/2406.04692), [Repo](https://github.com/togethercomputer/moa)|
| (3) **McEval: Massively Multilingual Code Evaluation** - This paper proposes a massively multilingual code benchmark covering 40 programming languages (McEval) with 16K test samples, which substantially pushes the limits of code LLMs in multilingual scenarios. The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct. In addition, the authors introduce an effective multilingual coder mCoder trained on McEval-Instruct to support multilingual programming language generation. Extensive experimental results on McEval show that there is still a difficult journey between open-source models and closed-source LLMs (e.g. GPT-series models) in numerous languages. | June 11, 2024 | [Paper](https://arxiv.org/abs/2406.07436), [Repo](https://github.com/McEval/McEval)|
| (4) **What If We Recaption Billions of Web Images with LLaMA-3?** - This paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM. The recaptioning pipeline is simple: first, the authors fine-tune a LLaMA-3-8B powered LLaVA1.5 and then employ it to recaption ∼1.3 billion images from the DataComp-1B dataset. The empirical results confirm that this enhanced dataset, Recap-DataComp1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, the authors observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users’ text instructions, especially in following complex queries. | June 12, 2024 | [Paper](https://arxiv.org/abs/2406.08478), [Repo](https://github.com/UCSC-VLAA/Recap-DataComp-1B)|
| (5) **Depth Anything V2** - This paper presents Depth Anything V2. Without pursuing fancy techniques, the authors aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. The authors offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, the authors fine-tune them with metric depth labels to obtain our metric depth models. In addition to the models, considering the limited diversity and frequent noise in current test sets, the authors construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research. | June 13, 2024 | [Paper](https://arxiv.org/abs/2406.09414), [Repo](https://github.com/DepthAnything/Depth-Anything-V2)|
| (6) **4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities** - This paper presents a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. This includes training on images and text along with several semantic and geometric modalities, feature maps from recent state of the art models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example, image metadata or color palettes. A crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text. Through this, the authors show the possibility of training one model to solve at least 3x more tasks/modalities than existing models and doing so without a loss in performance. In addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model. The authors scale the training to a three billion parameter and different datasets. | June 14, 2024 | [Paper](https://arxiv.org/abs/2406.09406), [Repo](https://github.com/apple/ml-4m/)|
| (7) **Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation** - This paper introduces LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. The authors reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) The authors verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. | June 10, 2024 | [Paper](https://arxiv.org/abs/2406.06525), [Repo](https://github.com/FoundationVision/LlamaGen)|


## AI Papers of the Week (June 3 - June 9) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis** - This paper introduces Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. This work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 256 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, the authors extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVANeXT-Video. The experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models with an average accuracy of 75.7%, compared to 52.5% for LLaVA-NeXT-Video. The results also demonstrate that Video-MME is a universal benchmark, which applies to both image and video MLLMs. Further analysis indicates that subtitle and audio information could significantly enhance video understanding. Besides, a decline in MLLM performance is observed as video duration increases for all models. This dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data, shedding light on future MLLM development. | May 31, 2024 | [Paper](https://arxiv.org/abs/2405.21075), [Repo](https://github.com/BradyFU/Video-MME)|
| (2) **To Believe or Not to Believe Your LLM** - This paper explores uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. The authors simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, the authors derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. The authors conduct a series of experiments which demonstrate the advantage of our formulation. Further, this investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest. | June 4, 2024 | [Paper](https://arxiv.org/abs/2406.02543)|
| (3) **I4VGen: Image as Stepping Stone for Text-to-Video Generation** - This paper presents I4VGen, a training-free and plug-and-play video diffusion inference framework, which enhances text-to-video generation by leveraging robust image techniques. Specifically, following text-to-image-to-video, I4VGen decomposes the text-to-video generation into two stages: anchor image synthesis and anchor image-guided video synthesis. Correspondingly, a well-designed generation-selection pipeline is employed to achieve visually-realistic and semantically-faithful anchor image, and an innovative Noise-Invariant Video Score Distillation Sampling is incorporated to animate the image to a dynamic video, followed by a video regeneration process to refine the video. This inference strategy effectively mitigates the prevalent issue of non-zero terminal signal-to-noise ratio. Extensive evaluations show that I4VGen not only produces videos with higher visual realism and textual fidelity but also integrates seamlessly into existing image-to-video diffusion models, thereby improving overall video quality. | June 4, 2024 | [Paper](https://arxiv.org/abs/2406.02230), [Repo](https://xiefan-guo.github.io/i4vgen/)|
| (4) **ShareGPT4Video: Improving Video Understanding and Generation with Better Captions** - This paper presents the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-tovideo models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, the authors find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. The authors argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, the authors meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, the authors construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, the authors further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, the authors verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. | June 6, 2024 | [Paper](https://arxiv.org/abs/2406.04325), [Repo](https://github.com/ShareGPT4Omni/ShareGPT4Video)|
| (5) **Are We Done with MMLU?** - This paper identifies and analyses errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, this analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, the authors find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, the authors introduce a comprehensive framework for identifying dataset errors using a novel error taxonomy. Then, the authors create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects. Using MMLURedux, the authors demonstrate significant discrepancies with the model performance metrics that were originally reported. The results strongly advocate for revising MMLU’s error-ridden questions to enhance its future utility and reliability as a benchmark. | June 6, 2024 | [Paper](https://arxiv.org/abs/2406.04127)|
| (6) **Parrot: Multilingual Visual Instruction Tuning** - This paper introduces Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level. Parrot makes the visual tokens condition on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens. Specifically, to enhance non-English visual tokens alignment, the authors compute the cross-attention using the initial visual features and textual embeddings, the result of which is then fed into the MoE router to select the most relevant experts. The selected experts subsequently convert the initial visual tokens into language-specific visual tokens. Moreover, considering the current lack of benchmarks for evaluating multilingual capabilities within the field, the authors collect and make available a Massive Multilingual Multimodal Benchmark which includes 6 languages, 15 categories, and 12,000 questions, named as MMMB. This method not only demonstrates state-of-the-art performance on multilingual MMBench and MMMB, but also excels across a broad range of multimodal tasks. | June 4, 2024 | [Paper](https://arxiv.org/abs/2406.02539)|
| (7) **Verbalized Machine Learning: Revisiting Machine Learning with Language Models** - This paper introduces the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, the authors revisit classical machine learning problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a concrete model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why each learner update is performed. The authors conduct several studies to empirically evaluate the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability and trustworthiness in ML.. | June 6, 2024 | [Paper](https://arxiv.org/abs/2406.04344)|


## AI Papers of the Week (May 27 - June 2) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach** - In this work, the authors consider the problem of automatic curation of high-quality datasets for self-supervised pre-training. The authors posit that such datasets should be large, diverse and balanced, and propose a clustering-based approach for building ones satisfying all these criteria. This method involves successive and hierarchical applications of k-means on a large and diverse data repository to obtain clusters that distribute uniformly among data concepts, followed by a hierarchical, balanced sampling step from these clusters. Extensive experiments on three different data domains including web-based images, satellite images and text show that features trained on our automatically curated datasets outperform those trained on uncurated data while being on par or better than ones trained on manually curated data. | May 24, 2024 | [Paper](https://arxiv.org/abs/2405.15613), [Repo](https://github.com/facebookresearch/ssl-data-curation)|
| (2) **LLM Evaluators Recognize and Favor Their Own Generations** - This paper investigates if self-recognition capability contributes to self-preference. The authors discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By finetuning LLMs, the authors discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, the authors show that the causal explanation resists straightforward confounders. The authors discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally. | April 15, 2024 | [Paper](https://arxiv.org/abs/2404.13076)|
| (3) **ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models** - This paper proposes ConvLLaVA, which employs ConvNeXt, a hierarchical backbone, as the visual encoder of LMM to replace Vision Transformer (ViT). ConvLLaVA compresses high-resolution images into information-rich visual features, effectively preventing the generation of excessive visual tokens. To enhance the capabilities of ConvLLaVA, the authors propose two critical optimizations. Since the low-resolution pretrained ConvNeXt underperforms when directly applied on high resolution, the authors update it to bridge the gap. Moreover, since ConvNeXt’s original compression ratio is inadequate for much higher resolution inputs, the authors train a successive stage to further compress the visual tokens, thereby reducing redundancy. These optimizations enable ConvLLaVA to support inputs of 1536×1536 resolution generating only 576 visual tokens, capable of handling images of arbitrary aspect ratios. Experimental results demonstrate that this method achieves competitive performance with state-of-the-art models on mainstream benchmarks. | May 24, 2024 | [Paper](https://arxiv.org/abs/2405.15738), [Repo](https://github.com/alibaba/conv-llava)|
| (4) **M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models** - This paper introduces M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 8,931 samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German. Using M4U, the authors conduct extensive evaluations of 21 leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results show that the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy on M4U. Additionally, the authors observe that the leading LMMs exhibit significant language preferences. This in-depth analysis indicates that leading LMMs, including GPT-4o, suffer performance degradation when prompted with cross-lingual multimodal questions, such as images with key textual information in Chinese while the question is in German. The authors believe that M4U can serve as a crucial tool for systematically evaluating LMMs based on their multilingual multimodal reasoning capabilities and monitoring their development. | May 24, 2024 | [Paper](https://arxiv.org/abs/2405.15638), [Repo](https://github.com/M4U-Benchmark/M4U)|
| (5) **Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts** - This paper argues that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation. To this end, the authors propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework. To begin with, beyond existing similarity oriented thought, the authors embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts. Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, the authors propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought. Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag. | May 30, 2024 | [Paper](https://arxiv.org/abs/2405.19893)|
| (6) **An Introduction to Vision-Language Modeling** - This paper present this introduction to VLMs to better understand the mechanics behind mapping vision to language. The authors hope this will help anyone who would like to enter the field. First, the authors introduce what VLMs are, how they work, and how to train them. Then, the authors present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, the authors also discuss extending VLMs to videos. | May 27, 2024 | [Paper](https://arxiv.org/abs/2405.19893)|
| (7) **MotionLLM: Understanding Human Behaviors from Human Motions and Videos** - This study delves into the realm of multi-modality (i.e., video and motion modalities) human behavior understanding by leveraging the powerful capabilities of Large Language Models (LLMs). Diverging from recent LLMs designed for video-only or motion-only understanding, the authors argue that understanding human behavior necessitates joint modeling from both videos and motion sequences (e.g., SMPL sequences) to capture nuanced body part dynamics and semantics effectively. In light of this, the authors present MotionLLM, a straightforward yet effective framework for human motion understanding, captioning, and reasoning. Specifically, MotionLLM adopts a unified video-motion training strategy that leverages the complementary advantages of existing coarse video-text data and fine-grained motion-text data to glean rich spatial-temporal insights. Furthermore, the authors collect a substantial dataset, MoVid, comprising diverse videos, motions, captions, and instructions. Additionally, the authors propose the MoVid-Bench, with carefully manual annotations, for better evaluation of human behavior understanding on video and motion. Extensive experiments show the superiority of MotionLLM in the caption, spatial-temporal comprehension, and reasoning ability. | May 30, 2024 | [Paper](https://arxiv.org/abs/2405.20340), [Repo](https://github.com/IDEA-Research/MotionLLM)|


## AI Papers of the Week (May 20 - May 26) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Many-Shot In-Context Learning in Multimodal Foundation Models** - The paper evaluates the performance of multimodal foundation models scaling from few-shot to many-shot ICL. The authors benchmark GPT-4o and Gemini 1.5 Pro across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We observe that many-shot ICL, including up to almost 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, the authors also explore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and many–shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, the authors measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. The authors find that while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. The results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. | May 16, 2024 | [Paper](https://arxiv.org/abs/2405.09798), [Repo](https://github.com/stanfordmlgroup/ManyICL)|
| (2) **FIFO-Diffusion: Generating Infinite Videos from Text without Training** - The paper proposes a novel inference technique based on a pretrained diffusion model for text-conditional video generation. This approach, called FIFO-Diffusion, is conceptually capable of generating infinitely long videos without training. This is achieved by iteratively performing diagonal denoising, which concurrently processes a series of consecutive frames with increasing noise levels in a queue; this method dequeues a fully denoised frame at the head while enqueuing a new random noise frame at the tail. However, diagonal denoising is a double-edged sword as the frames near the tail can take advantage of cleaner ones by forward reference but such a strategy induces the discrepancy between training and inference. Hence, the authors introduce latent partitioning to reduce the training-inference gap and lookahead denoising to leverage the benefit of forward referencing. The authors have demonstrated the promising results and effectiveness of the proposed methods on existing textto-video generation baselines. | May 19, 2024 | [Paper](https://arxiv.org/abs/2405.11473), [Repo](https://github.com/jjihwan/FIFO-Diffusion_public)|
| (3) **MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning** - The paper analyzes the impact of low-rank updating, as implemented in LoRA. The findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, the authors propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, the authors introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. The authors perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. This method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks. | May 20, 2024 | [Paper](https://arxiv.org/abs/2405.12130), [Repo](https://github.com/kongds/MoRA)|
| (4) **RAFT: Adapting Language Model to Domain Specific RAG** - The paper presents Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, the authors train the model to ignore those documents that don't help in answering the question, which they call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. | May 20, 2024 | [Paper](https://arxiv.org/abs/2403.10131), [Repo](https://github.com/ShishirPatil/gorilla)|
| (5) **Aya 23: Open Weight Releases to Further Multilingual Progress** - This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model, focusing on pairing a highly performant pre-trained model with the recently released Aya collection. The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world’s population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. | May 20, 2024 | [Paper](https://cohere.com/research/papers/aya-command-23-8b-and-35b-technical-report-2024-05-23), [Repo](https://huggingface.co/CohereForAI)|
| (6) **Not All Language Model Features Are Linear** - This work explores whether some language model representations may be inherently multi-dimensional. The authors begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, the authors design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. The authors identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Finally, the authors provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we find further circular representations by breaking down the hidden states for these tasks into interpretable components. | May 23, 2024 | [Paper](https://arxiv.org/abs/2405.14860)|
| (7) **ReVideo: Remake a Video with Motion and Content Control** - This paper presents a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, the authors develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, the authors propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. This method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness. | May 22, 2024 | [Paper](https://arxiv.org/abs/2405.13865), [Repo](https://github.com/MC-E/ReVideo)|


## AI Papers of the Week (May 13 - May 19) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **What matters when building vision-language models?** - The paper conducts extensive experiments around pre-trained models, architecture choice, data, and training methods. The consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. The authors release the model (base, instructed, and chat) along with the datasets created for its training. | May 3, 2024 | [Paper](https://arxiv.org/abs/2405.02246), [Repo](https://huggingface.co/collections/HuggingFaceM4/idefics2-661d1971b7c50831dd3ce0fe)|
| (2) **Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity** - The paper proposes a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. The authors validate the model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. | March 28, 2024 | [Paper](https://arxiv.org/abs/2403.14403), [Repo](https://github.com/starsuzi/Adaptive-RAG)|
| (3) **Compositional Text-to-Image Generation with Dense Blob Representations** - The paper proposes to decompose a scene into visual primitives – denoted as dense blob representations – that contain fine-grained details of the scene while being modular, human-interpretable, and easy-to-construct. Based on blob representations, the authors develop a blob-grounded text-to-image diffusion model, termed BlobGEN, for compositional generation. Particularly, they introduce a new masked cross-attention module to disentangle the fusion between blob representations and visual features. To leverage the compositionality of large language models (LLMs), the authors introduce a new in-context learning approach to generate blob representations from text prompts. The extensive experiments show that BlobGEN achieves superior zero-shot generation quality and better layout-guided controllability on MS-COCO. When augmented by LLMs, this method exhibits superior numerical and spatial correctness on compositional image generation benchmarks. | May 14, 2024 | [Paper](https://arxiv.org/abs/2405.08246), [Repo](https://blobgen-2d.github.io/)|
| (4) **CLIP with Quality Captions: A Strong Pretraining for Vision Tasks** - The paper finds that simply improving the quality of captions in image-text datasets improves the quality of CLIP’s visual representations, resulting in significant improvement on downstream dense prediction vision tasks. In fact, the authors find that CLIP pretraining with good quality captions can surpass recent supervised, self-supervised and weakly supervised pretraining methods. They show that when CLIP model with ViT-B/16 as image encoder is trained on well aligned image-text pairs it obtains 12.1% higher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation tasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining methods like Masked Autoencoder (MAE). The authors find that mobile architectures also benefit significantly from CLIP pretraining. A recent mobile vision architecture, MCi2, with CLIP pretraining obtains similar performance as Swin-L, pretrained on ImageNet-22k for semantic segmentation task while being 6.1× smaller. Moreover, the study shows that improving caption quality results in 10× data efficiency when finetuning for dense prediction tasks. | May 14, 2024 | [Paper](https://arxiv.org/abs/2405.08911)|
| (5) **CLLMs: Consistency Large Language Models** - Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, the authors develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4× to 3.4× improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks. | May 8, 2024 | [Paper](https://arxiv.org/abs/2403.00835), [Repo](https://github.com/hao-ai-lab/Consistency_LLM)|
| (6) **Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection** - The paper introduces a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. This framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. | October 17, 2023 | [Paper](https://arxiv.org/abs/2310.11511)|
| (7) **LoRA Learns Less and Forgets Less** - The paper compares the performance of LoRA and full finetuning on two target domains, programming and mathematics. The authors consider both the instruction finetuning (≈100K prompt-response pairs) and continued pretraining (≈10B unstructured tokens) data regimes. The results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model's performance on tasks outside the target domain. The results show that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. The authors show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. The authors conclude by proposing best practices for finetuning with LoRA. | May 15, 2024 | [Paper](https://arxiv.org/abs/2405.09673)|


## AI Papers of the Week (May 6 - May 12) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory** - The paper draws attention to the highly critical yet overlooked notion of contextual privacy by proposing CONFAIDE, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. The experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when the authors employ privacy-inducing prompts or chain-of-thought reasoning. The work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind. | October 27, 2023 | [Paper](https://arxiv.org/abs/2310.17884), [Repo](https://github.com/skywalker023/confAIde)|
| (2) **A Careful Examination of Large Language Model Performance on Grade School Arithmetic** - The paper commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. the authors ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, the authors observe accuracy drops of up to 13%, with several families of models (e.g. Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g. Gemini/GPT/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman’s r 2 = 0.32) between a model’s probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that many models may have partially memorized GSM8k. | May 3, 2024 | [Paper](https://arxiv.org/abs/2405.00332)|
| (3) **LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report** - The paper aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, the authors measured the quality of LLMs fine-tuned with quantized low-rank adapters across 10 base models and 31 tasks for a total of 310 models. They find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, the authors investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, the authors evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM. | April 29, 2024 | [Paper](https://arxiv.org/abs/2405.00732)|
| (4) **Better & Faster Large Language Models via Multi-token Prediction** - The paper suggests that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, the authors ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, the authors measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where the models consistently outperform strong baselines by several percentage points. The 13B parameter models solves 12 % more problems on HumanEval and 17% more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3× faster at inference, even with large batch sizes. | April 30, 2024 | [Paper](https://arxiv.org/abs/2404.19737)|
| (5) **Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation** - The paper proposes VisualFactChecker (VFC), a flexible training-free pipeline that generates high-fidelity and detailed captions for both 2D images and 3D objects. VFC consists of three steps: 1) proposal, where image-to-text captioning models propose multiple initial captions; 2) verification, where a large language model (LLM) utilizes tools such as object detection and VQA models to fact-check proposed captions; 3) captioning, where an LLM generates the final caption by summarizing caption proposals and the fact check verification results. In this step, VFC can flexibly generate captions in various styles following complex instructions. The authors conduct comprehensive captioning evaluations using four metrics: 1) CLIP-Score for image-text similarity; 2) CLIP-Image-Score for measuring the image-image similarity between the original and the reconstructed image generated by a text-to-image model using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V for fine-grained evaluation. Evaluation results show that VFC outperforms state-of-the-art open-sourced captioning methods for 2D images on the COCO dataset and 3D assets on the Objaverse dataset. The study demonstrates that by combining open-source models into a pipeline, we can attain captioning capability comparable to proprietary models such as GPT-4V, despite being over 10x smaller in model size. | April 30, 2024 | [Paper](https://arxiv.org/abs/2404.19752)|
| (6) **Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?** - The paper studies the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, the authors design a controlled setup, focused on closed-book QA, where they vary the proportion of the fine-tuning examples that introduce new knowledge. The authors demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model’s knowledge. However, the authors also find that as the examples with new knowledge are eventually learned, they linearly increase the model’s tendency to hallucinate. Taken together, the results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas finetuning teaches them to use it more efficiently. | May 9, 2024 | [Paper](https://arxiv.org/abs/2405.05904)|
| (7) **Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3** - The paper presents a targeted model editing analysis focused on the latest large language model, Llama-3. The authors explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions. They identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing. The findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits. With this, the authors argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing. This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and the authors hope it paves way for future investigations into optimizing batch sizes and model editing performance. | May 1, 2024 | [Paper](https://arxiv.org/abs/2405.00664)|


## AI Papers of the Week (April 29 - May 5) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs** - The paper introduces AdvPrompter, a method to rapidly generate human-readable adversarial prompts to expose vulnerabilities in Large Language Models (LLMs). This approach, which is about 800 times faster than previous methods, does not require gradient information from the target LLM. It involves a two-step training process using high-quality adversarial suffixes and low-rank fine-tuning. The trained AdvPrompter is effective in deceiving LLMs into producing harmful responses, improving the robustness of LLMs against adversarial attacks. | April 21, 2024 | [Paper](https://arxiv.org/abs/2404.16873)|
| (2) **Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models** - The paper proposes using a panel of smaller language models (PoLL) to evaluate the output of large language models (LLMs) instead of a single large model. This approach aims to reduce cost and bias while improving evaluation accuracy. The authors demonstrate that a diverse panel of models provides better, less biased assessments and is more cost-effective than using a single large model for evaluating LLMs.  | April 29, 2024 | [Paper](https://arxiv.org/abs/2404.18796)|
| (3) **Extending Llama-3’s Context Ten-Fold Overnight** - The study extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning2 . The entire training cycle is super efficient, which takes 8 hours on one 8xA800 (80G) GPU machine. The resulted model exhibits superior performances across a broad range of evaluation tasks, such as NIHS, topic retrieval, and longcontext language understanding; meanwhile, it also well preserves the original capability over short contexts. The dramatic context extension is mainly attributed to merely 3.5K synthetic training samples generated by GPT-4, which indicates the LLMs’ inherent (yet largely underestimated) potential to extend its original context length. In fact, the context length could be extended far beyond 80K with more computation resources.  | April 30, 2024 | [Paper](https://arxiv.org/abs/2404.19553), [Repo](https://github.com/FlagOpen/FlagEmbedding)|
| (4) **Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models** - This paper introduces Vibe-Eval: a new open benchmark and framework for evaluating multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts, including 100 of hard difficulty, complete with gold-standard responses authored by experts. Vibe-Eval is open-ended and challenging with dual objectives: (i) vibe checking multimodal chat models for day-to-day tasks and (ii) rigorously testing and probing the capabilities of present frontier models. Notably, the hard set contains > 50% questions that all frontier models answer incorrectly. They explore the nuances of designing, evaluating, and ranking models on ultra challenging prompts. They also discuss trade-offs between human and automatic evaluation, and show that automatic model evaluation using Reka Core roughly correlates to human judgment.  | May 1, 2024 | [Paper](https://publications.reka.ai/reka-vibe-eval.pdf), [Repo](https://github.com/reka-ai/reka-vibe-eval)|
| (5) **PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models** - This paper introduces introduce Prometheus 2, a more powerful evaluator LM than it’s predecessor that closely mirrors human and GPT-4 judgements. The model aim to address the critical shortcomings of proprietary LMs: 1) they issue scores that significantly diverge from those assigned by humans, 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment, and 3) they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. The model is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, PROMETHEUS 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs.  | May 2, 2024 | [Paper](https://arxiv.org/abs/2405.01535), [Repo](https://github.com/prometheus-eval/prometheus-eval)|
| (6) **FLAME: Factuality-Aware Alignment for Large Language Models** - This paper studies how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, they find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human-labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, the authors propose factuality-aware alignment (FLAME), comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that the proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.  | May 2, 2024 | [Paper](https://arxiv.org/abs/2405.01525)|
| (7) **WildChat: 1M ChatGPT Interaction Logs in the Wild** - This paper bridges the gap of understanding how Chatbots such as GPT-4 used by a population of users in practice. The research compiled WILDCHAT, a corpus of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns. They compare WILDCHAT with other popular user-chatbot interaction datasets, and find that this dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In addition to timestamped chat transcripts, the authors enrich the dataset with demographic data, including state, country, and hashed IP addresses, alongside request headers. This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions. Finally, because it captures a broad range of use cases, the authors demonstrate the dataset’s potential utility in fine-tuning instruction-following models. | May 2, 2024 | [Paper](https://arxiv.org/abs/2405.01470), [Repo](https://wildchat.allen.ai/)|


## AI Papers of the Week (April 22 - April 28) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **Many-Shot In-Context Learning** - The paper explores the transition from few-shot to many-shot in-context learning (ICL) for large language models (LLMs), demonstrating substantial performance gains across a range of tasks. It introduces two novel settings: Reinforced ICL, using model-generated rationales instead of human examples, and Unsupervised ICL, which omits rationales and relies solely on domain-specific questions. These methods significantly enhance the model's ability to tackle complex reasoning tasks and overcome pretraining biases, particularly effective in the many-shot regime. The findings also highlight the limitations of next-token prediction loss as an indicator of downstream ICL performance. | April 17, 2024 | [Paper](https://arxiv.org/abs/2404.11018)|
| (2) **Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone** - The paper introduces phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench). | April 22, 2024 | [Paper](https://arxiv.org/abs/2404.14219)|
| (3) **The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions** - The paper argues that one of the primary vulnerabilities underlying these attacks on LLMs is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, they propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. Then proposes an automated data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. They apply this method to LLMs, showing that it drastically increases robustness—even for attack types not seen during training—while imposing minimal degradations on standard capabilities. | April 19, 2024 | [Paper](https://arxiv.org/abs/2404.13208)|
| (4) **OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework** - This work releases OpenELM, a decoder-only transformer-based open language model. The OpenELM uses a layer-wise scaling method for efficient parameter allocation within the transformer model, resulting in improved accuracy compared to existing models. Additionally, they have made the entire framework open-source, including training logs, multiple checkpoints, pre-training configurations, and MLX inference code. This extensive release aims to empower and strengthen the open research community, facilitating future research efforts. | April 22, 2024 | [Paper](https://arxiv.org/abs/2404.14619), [Repo](https://github.com/apple/corenet?tab=readme-ov-file)|
| (5) **Multi-Head Mixture-of-Experts** - This work introduces a new architecture enhancing Sparse Mixtures of Experts (SMoE) by integrating a multi-head mechanism. This mechanism divides each token into sub-tokens, which are then processed by a diverse set of experts in parallel. The sub-tokens are subsequently recombined, enriching the model's capacity to process multiple semantic concepts simultaneously. This approach significantly improves expert activation, deepening contextual understanding and reducing overfitting, and is easy to implement alongside other SMoE models. The effectiveness of MH-MoE is demonstrated across various multilingual and multimodal tasks. | April 23, 2024 | [Paper](https://arxiv.org/abs/2404.15045)|
| (6) **Chinchilla Scaling: A replication attempt** - This work examines the computational scaling laws proposed by Hoffmann et al. (2022). They attempt to replicate the third estimation method from Hoffmann's work, which involves fitting a parametric loss function to a reconstruction of data from their plots. The authors find that the reported estimates are inconsistent with Hoffmann's first two estimation methods, fail at fitting the extracted data, and report implausibly narrow confidence intervals—intervals this narrow would require over 600,000 experiments, while Hoffmann's study likely only ran fewer than 500. In contrast, the rederivation of the scaling law using the third approach yields results that are compatible with the findings from the first two estimation procedures described by Hoffmann et al.  | April 15, 2024 | [Paper](https://arxiv.org/abs/2404.10102), [Repo](https://epochai.org/blog/chinchilla-scaling-a-replication-attempt)|
| (7) **How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites** - This work introduces InternVL 1.5, an opensource multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. They introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model—InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448×448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chineserelated tasks. They evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.  | April 25, 2024 | [Paper](https://arxiv.org/abs/2404.16821), [Repo](https://github.com/OpenGVLab/InternVL)|


## AI Papers of the Week (April 15 - April 21) - 2024
| **Paper**  | **Release Date** | **Links** |
| ------------- | ------------- | ------------- |
| (1) **ReFT: Representation Finetuning for Language Models** - The paper introduces Representation Finetuning (ReFT) methods that operate on a frozen base model to learn task-specific interventions on hidden representations. Additionally, it defines a robust instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), which serves as a drop-in replacement for existing Parameter-efficient fine-tuning (PEFT) methods and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. It showcases LoReFT across eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. | April 4, 2024 | [Paper](https://arxiv.org/abs/2404.03592), [Repo](https://github.com/stanfordnlp/pyreft)|
| (2) **No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance** - This work addresses the question: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? The research investigates this across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. It finds that multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. Additionally, the study introduces a long-tail test set, the "Let it Wag!" benchmark, to encourage further research. Overall, the findings highlight a substantial need for training data to enhance "zero-shot" generalization capabilities in multimodal models under large-scale training paradigms.  | April 4, 2024 | [Paper](https://arxiv.org/abs/2404.04125), [Repo](https://github.com/bethgelab/frequency_determines_performance)|
| (3) **Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention** - This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to handle infinitely long inputs using bounded memory and computation. The proposed Infini-attention technique enhances the traditional attention mechanism by incorporating compressive memory, masked local attention, and long-term linear attention into a single Transformer block. The effectiveness of this approach is demonstrated through its application in long-context language modeling benchmarks, 1M sequence length passkey context block retrieval, and 500K length book summarization tasks with 1B and 8B LLMs, enabling fast streaming inference with minimal memory overhead.  | April 10, 2024 | [Paper](https://arxiv.org/abs/2404.07143)|
| (4) **Rho-1: Not All Tokens Are What You Need** - This work challenges traditional language model training methods that uniformly apply next-token prediction loss to all tokens. It argues that not all tokens are equally important and introduces a new model, Rho-1, which uses Selective Language Modeling (SLM) to focus on tokens that align better with the desired distribution. Rho-1 scores pretraining tokens using a reference model and trains the language model to focus on tokens with higher excess loss. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.  | April 11, 2024 | [Paper](https://arxiv.org/abs/2404.07965#:~:text=Previous%20language%20model%20pre%2Dtraining,important%20for%20language%20model%20training%22), [Repo](https://github.com/microsoft/rho)|
| (5) **CodecLM: Aligning Language Models with Tailored Synthetic Data** - This work presents CodecLM, a framework for adapting large language models (LLMs) to specific task instructions using tailored synthetic data. CodecLM improves instruction-following capabilities in LLMs by selectively generating high-quality data aligned with target instruction distributions. The approach uses an encode-decode mechanism with Self-Rubrics and Contrastive Filtering, achieving superior performance over existing models in multiple instruction-following benchmarks.  | April 8, 2024 | [Paper](https://arxiv.org/abs/2404.05875)|
| (6) **ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback** - The paper presents an advanced approach to enhancing the controllability of text-to-image diffusion models. ControlNet++ improves upon its predecessor by optimizing pixel-level cycle consistency between generated images and input conditions using a novel reward strategy. This efficient method minimizes the memory and time costs associated with traditional image sampling techniques. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions. | April 11, 2024 | [Paper](https://arxiv.org/abs/2404.07987#:~:text=Extensive%20experiments%20show%20that%20ControlNet,art%20edge%2C%20and%20depth%20conditions.), [Repo](https://github.com/liming-ai/ControlNet_Plus_Plus)|
| (7) **VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time** - The paper presents VASA-1, a framework for generating realistic talking faces from a single static image and a speech audio clip. VASA-1 produces detailed lip movements and captures a wide range of facial expressions and head motions, enhancing the perception of authenticity and liveliness. It surpasses existing methods in video quality and facial dynamics and supports real-time generation of 512x512 videos at up to 40 FPS with minimal starting latency, marking a significant advancement in real-time interactive systems using lifelike avatars. | April 16, 2024 | [Paper](https://arxiv.org/abs/2404.10667), [Repo](https://www.microsoft.com/en-us/research/project/vasa-1/)|
| (8) **Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video** - The paper presents Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments. At the heart of the system are three core components: (i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among objects. The carefully designed pipeline enables the construction of interactable and actionable digital replicas of the real world. The research benchmarks the system on both indoor and large-scale outdoor scenes. They demonstrate that they can produce highly realistic renderings in real-time and build interactive games on top. | April 15, 2024 | [Paper](https://arxiv.org/abs/2404.09833), [Repo](https://github.com/video2game/video2game)|
| (9) **HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing** - The paper introduces HQ-Edit, a dataset designed to enhance image editing models through instruction-based edits. It features around 200,000 edits, created using advanced models like GPT-4V and DALL-E 3. The dataset emphasizes high quality and diversity with detailed text prompts for each image pair, facilitated by a scalable data collection pipeline. In addition, they propose two evaluation metrics, Alignment and Coherence, to quantitatively assess the quality of image edit pairs using GPT-4V. HQ-Edits high-resolution images, rich in detail and accompanied by comprehensive editing prompts, substantially enhance the capabilities of existing image editing models. For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image editing performance, even surpassing those models fine-tuned with human-annotated data. | April 15, 2024 | [Paper](https://arxiv.org/abs/2404.09990), [Repo](https://thefllood.github.io/HQEdit_web/)|
